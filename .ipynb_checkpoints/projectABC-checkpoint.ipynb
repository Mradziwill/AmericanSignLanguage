{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62fc197f",
   "metadata": {},
   "source": [
    "## Import required Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beba1683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b209c7a",
   "metadata": {},
   "source": [
    "## Usefull functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6607a0e",
   "metadata": {},
   "source": [
    "### function showing progres in learning of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d864700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(history):\n",
    "  \"\"\"\n",
    "  Returns separate loss curves for training and validation metrics.\n",
    "\n",
    "  Args:\n",
    "    history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n",
    "  \"\"\" \n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  accuracy = history.history['accuracy']\n",
    "  val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "  epochs = range(len(history.history['loss']))\n",
    "\n",
    "  # Plot loss\n",
    "  plt.plot(epochs, loss, label='training_loss')\n",
    "  plt.plot(epochs, val_loss, label='val_loss')\n",
    "  plt.title('Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend()\n",
    "\n",
    "  # Plot accuracy\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "  plt.title('Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ef7677",
   "metadata": {},
   "source": [
    "### function to make preatty confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecdc0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Our function needs a different name to sklearn's plot_confusion_matrix\n",
    "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False): \n",
    "\n",
    "  # Create the confustion matrix\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
    "  n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
    "\n",
    "  # Plot the figure and make it pretty\n",
    "  fig, ax = plt.subplots(figsize=figsize)\n",
    "  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
    "  fig.colorbar(cax)\n",
    "\n",
    "  # Are there a list of classes?\n",
    "  if classes:\n",
    "    labels = classes\n",
    "  else:\n",
    "    labels = np.arange(cm.shape[0])\n",
    "  \n",
    "  # Label the axes\n",
    "  ax.set(title=\"Confusion Matrix\",\n",
    "         xlabel=\"Predicted label\",\n",
    "         ylabel=\"True label\",\n",
    "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
    "         yticks=np.arange(n_classes), \n",
    "         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
    "         yticklabels=labels)\n",
    "  \n",
    "  # Make x-axis labels appear on bottom\n",
    "  ax.xaxis.set_label_position(\"bottom\")\n",
    "  ax.xaxis.tick_bottom()\n",
    "\n",
    "  ### Added: Rotate xticks for readability & increase font size (required due to such a large confusion matrix)\n",
    "  plt.xticks(rotation=70, fontsize=text_size)\n",
    "  plt.yticks(fontsize=text_size)\n",
    "\n",
    "  # Set the threshold for different colors\n",
    "  threshold = (cm.max() + cm.min()) / 2.\n",
    "\n",
    "  # Plot the text on each cell\n",
    "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    if norm:\n",
    "      plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
    "              horizontalalignment=\"center\",\n",
    "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "              size=text_size)\n",
    "    else:\n",
    "      plt.text(j, i, f\"{cm[i, j]}\",\n",
    "              horizontalalignment=\"center\",\n",
    "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "              size=text_size)\n",
    "\n",
    "  # Save the figure to the current working directory\n",
    "  if savefig:\n",
    "    fig.savefig(\"confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939054a",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4537e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './gestureABC/train'\n",
    "test_path = './gestureABC/test'\n",
    "valid_path = './gestureABC/validate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f163ed58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1800 files belonging to 3 classes.\n",
      "Found 60 files belonging to 3 classes.\n",
      "Found 60 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Setup data inputs\n",
    "import tensorflow as tf\n",
    "IMG_SIZE = (64, 64)                              \n",
    "\n",
    "#shuffle train dataset\n",
    "train = tf.keras.preprocessing.image_dataset_from_directory(train_path,\n",
    "                                                                label_mode=\"categorical\",\n",
    "                                                                image_size=IMG_SIZE,\n",
    "                                                                shuffle=True)\n",
    "                                                                \n",
    "validate = tf.keras.preprocessing.image_dataset_from_directory(valid_path,\n",
    "                                                                        label_mode=\"categorical\",\n",
    "                                                                        image_size=IMG_SIZE,\n",
    "                                                                        shuffle=False)\n",
    "\n",
    "test = tf.keras.preprocessing.image_dataset_from_directory(test_path,\n",
    "                                                                        label_mode=\"categorical\",\n",
    "                                                                        image_size=IMG_SIZE,\n",
    "                                                                        shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8999cc",
   "metadata": {},
   "source": [
    "## Create checkpoint_callback to be able to continue training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29563044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorboard callback (functionized because need to create a new one for each model)\n",
    "import datetime\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir\n",
    "  )\n",
    "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "  return tensorboard_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decbc962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup checkpoint path\n",
    "checkpoint_path = \"./checkpointsABC/checkpoint.ckpt\" # note: remember saving directly to Colab is temporary\n",
    "\n",
    "# Create a ModelCheckpoint callback that saves the model's weights only\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True, # set to False to save the entire model\n",
    "                                                         save_best_only=True, # set to True to save only the best model instead of a model every epoch \n",
    "                                                         save_freq=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b657b0",
   "metadata": {},
   "source": [
    "## Create model using transfer learnining with EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab28f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data augmentation layer\n",
    "data_augmentation = Sequential([\n",
    "  preprocessing.RandomFlip('horizontal'),\n",
    "  preprocessing.RandomRotation(0.2),# keep for ResNet50V2, remove for EfficientNet  0.25                 \n",
    "], name=\"data_augmentation\")\n",
    "# add random distortion to pixels salt and pepper\n",
    "\n",
    "# Setup the input shape to our model\n",
    "input_shape = (64, 64, 3)\n",
    "\n",
    "# Create a frozen base model\n",
    "base_model = tf.keras.applications.EfficientNetB0(input_shape=(64, 64, 3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create input and output layers\n",
    "inputs = layers.Input(shape=input_shape, name=\"input_layer\") # create input layer\n",
    "x = data_augmentation(inputs) # augment our training images\n",
    "x = base_model(x, training=False) # pass augmented images to base model but keep it in inference mode, so batchnorm layers don't get updated: https://keras.io/guides/transfer_learning/#build-a-model \n",
    "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "outputs = layers.Dense(3, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da8bcee",
   "metadata": {},
   "source": [
    "## Train for 25 epocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de51c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: transfer_learning/ProjectEfficientNet/20230309-172107\n",
      "Epoch 1/25\n",
      "57/57 [==============================] - 9s 87ms/step - loss: 0.3401 - accuracy: 0.8950 - val_loss: 0.4440 - val_accuracy: 0.7833\n",
      "Epoch 2/25\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.0681 - accuracy: 0.9939 - val_loss: 0.2537 - val_accuracy: 0.9167\n",
      "Epoch 3/25\n",
      " 8/57 [===>..........................] - ETA: 3s - loss: 0.0434 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "history_25_epocks = model.fit(train,\n",
    "                                        \n",
    "                                          epochs=25,\n",
    "                                          validation_data=validate,\n",
    "                                          validation_steps=len(validate), # do less steps per validation (quicker)\n",
    "                                          callbacks=[create_tensorboard_callback(\"transfer_learning\", \"ProjectEfficientNet\"), \n",
    "                                                     checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842272c4",
   "metadata": {},
   "source": [
    "### Show the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469600f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history_25_epocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34292fa",
   "metadata": {},
   "source": [
    "## Save the model and then try to improve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437620c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_ABC_EfficientNetB0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d8ce9",
   "metadata": {},
   "source": [
    "## Evaluate it on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beba4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with model\n",
    "pred_probs = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class names\n",
    "class_names = validate.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e260bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class predicitons of each label\n",
    "pred_classes = pred_probs.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1af95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This might take a minute or so due to unravelling 790 batches\n",
    "y_labels = []\n",
    "for images, labels in test.unbatch(): # unbatch the test data and get images and labels\n",
    "  y_labels.append(labels.numpy().argmax()) # append the index which has the largest value (labels are one-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c247579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "sklearn_accuracy = accuracy_score(y_labels, pred_classes)\n",
    "sklearn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde6120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(y_true=y_labels,\n",
    "                      y_pred=pred_classes,\n",
    "                      classes=class_names,\n",
    "                      figsize=(50, 50),\n",
    "                      text_size=50,\n",
    "                      norm=False,\n",
    "                      savefig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# Get a dictionary of the classification report\n",
    "classification_report_dict = classification_report(y_labels, pred_classes, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808006ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create empty dictionary\n",
    "class_f1_scores = {}\n",
    "# Loop through classification report items\n",
    "for k, v in classification_report_dict.items():\n",
    "  if k == \"accuracy\": # stop once we get to accuracy key\n",
    "    break\n",
    "  else:\n",
    "    # Append class names and f1-scores to new dictionary\n",
    "    class_f1_scores[class_names[int(k)]] = v[\"f1-score\"]\n",
    "class_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749be25d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Turn f1-scores into dataframe for visualization\n",
    "import pandas as pd\n",
    "f1_scores = pd.DataFrame({\"class_name\": list(class_f1_scores.keys()),\n",
    "                          \"f1-score\": list(class_f1_scores.values())}).sort_values(\"f1-score\", ascending=False)\n",
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93eab9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 25))\n",
    "scores = ax.barh(range(len(f1_scores)), f1_scores[\"f1-score\"].values)\n",
    "ax.set_yticks(range(len(f1_scores)))\n",
    "ax.set_yticklabels(list(f1_scores[\"class_name\"]))\n",
    "ax.set_xlabel(\"f1-score\")\n",
    "ax.set_title(\"F1-Scores for 10 Different Classes\")\n",
    "ax.invert_yaxis(); # reverse the order\n",
    "\n",
    "def autolabel(rects): # Modified version of: https://matplotlib.org/examples/api/barchart_demo.html\n",
    "  \"\"\"\n",
    "  Attach a text label above each bar displaying its height (it's value).\n",
    "  \"\"\"\n",
    "  for rect in rects:\n",
    "    width = rect.get_width()\n",
    "    ax.text(1.03*width, rect.get_y() + rect.get_height()/1.5,\n",
    "            f\"{width:.2f}\",\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "autolabel(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15375fe1",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data augmentation layer\n",
    "data_augmentation = Sequential([\n",
    "  preprocessing.RandomFlip('horizontal'),\n",
    "  preprocessing.RandomRotation(0.2),# keep for ResNet50V2, remove for EfficientNet  0.25 \n",
    "  preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNet\n",
    "], name=\"data_augmentation\")\n",
    "# add random distortion to pixels salt and pepper\n",
    "\n",
    "# Setup the input shape to our model\n",
    "input_shape = (64, 64, 3)\n",
    "\n",
    "# Create a frozen base model\n",
    "base_model = tf.keras.applications.resnet50.ResNet50(input_shape=(64,64,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create input and output layers\n",
    "inputs = layers.Input(shape=input_shape, name=\"input_layer\") # create input layer\n",
    "x = data_augmentation(inputs) # augment our training images\n",
    "x = base_model(x, training=False) # pass augmented images to base model but keep it in inference mode, so batchnorm layers don't get updated: https://keras.io/guides/transfer_learning/#build-a-model \n",
    "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "outputs = layers.Dense(3, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8341793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_resnet = model.fit(train,\n",
    "                                        \n",
    "                                          epochs=25,\n",
    "                                          validation_data=validate,\n",
    "                                          validation_steps=len(validate), # do less steps per validation (quicker)\n",
    "                                          callbacks=[create_tensorboard_callback(\"transfer_learning\", \"ProjecResNet\"), \n",
    "                                                     checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with model\n",
    "pred_probs = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class predicitons of each label\n",
    "pred_classes = pred_probs.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab28fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This might take a minute or so due to unravelling 790 batches\n",
    "y_labels = []\n",
    "for images, labels in test.unbatch(): # unbatch the test data and get images and labels\n",
    "  y_labels.append(labels.numpy().argmax()) # append the index which has the largest value (labels are one-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818faeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "sklearn_accuracy = accuracy_score(y_labels, pred_classes)\n",
    "sklearn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158699ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(y_true=y_labels,\n",
    "                      y_pred=pred_classes,\n",
    "                      classes=class_names,\n",
    "                      figsize=(50, 50),\n",
    "                      text_size=50,\n",
    "                      norm=False,\n",
    "                      savefig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19271f18",
   "metadata": {},
   "source": [
    "## MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data augmentation layer\n",
    "data_augmentation = Sequential([\n",
    "  preprocessing.RandomFlip('horizontal'),\n",
    "  preprocessing.RandomRotation(0.2),# keep for ResNet50V2, remove for EfficientNet  0.25 \n",
    "  preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNet\n",
    "], name=\"data_augmentation\")\n",
    "# add random distortion to pixels salt and pepper\n",
    "\n",
    "# Setup the input shape to our model\n",
    "input_shape = (64, 64, 3)\n",
    "\n",
    "# Create a frozen base model\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(64,64,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create input and output layers\n",
    "inputs = layers.Input(shape=input_shape, name=\"input_layer\") # create input layer\n",
    "x = data_augmentation(inputs) # augment our training images\n",
    "x = base_model(x, training=False) # pass augmented images to base model but keep it in inference mode, so batchnorm layers don't get updated: https://keras.io/guides/transfer_learning/#build-a-model \n",
    "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "outputs = layers.Dense(3, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3f052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_MobileNetV2 = model.fit(train,\n",
    "                                        \n",
    "                                          epochs=25,\n",
    "                                          validation_data=validate,\n",
    "                                          validation_steps=len(validate), # do less steps per validation (quicker)\n",
    "                                          callbacks=[create_tensorboard_callback(\"transfer_learning\", \"ProjecMobileNetV2t\"), \n",
    "                                                     checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610da0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history_MobileNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247b1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with model\n",
    "pred_probs = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7217839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class predicitons of each label\n",
    "pred_classes = pred_probs.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This might take a minute or so due to unravelling 790 batches\n",
    "y_labels = []\n",
    "for images, labels in test.unbatch(): # unbatch the test data and get images and labels\n",
    "  y_labels.append(labels.numpy().argmax()) # append the index which has the largest value (labels are one-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_accuracy = accuracy_score(y_labels, pred_classes)\n",
    "sklearn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98666a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(y_true=y_labels,\n",
    "                      y_pred=pred_classes,\n",
    "                      classes=class_names,\n",
    "                      figsize=(50, 50),\n",
    "                      text_size=50,\n",
    "                      norm=False,\n",
    "                      savefig=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
